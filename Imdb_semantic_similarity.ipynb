{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWa/vFZss+LSCeBI1ubwii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shalakagangadhare/IMDB-semantic-similarity/blob/main/Imdb_semantic_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLP Project: Sentiment Analysis on IMDb Movie Reviews\n",
        "\n",
        "Introduction\n",
        "\n",
        "This project aims to build a model that can classify movie reviews from the IMDb dataset as either positive or negative. We will use Natural Language Processing (NLP) techniques to preprocess the text data, convert it into numerical features using TF-IDF, and then train a Logistic Regression model for classification.\n",
        "\n",
        "Dataset: The dataset contains 50,000 movie reviews, labeled as positive (1) or negative (0)."
      ],
      "metadata": {
        "id": "Gzw0hZd8xl6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§ª Results\n",
        "\n",
        "BERT-based embeddings significantly outperformed traditional TF-IDF and Word2Vec on semantic similarity.\n",
        "\n",
        "Models were able to identify similarity even when the vocabulary differed but sentiment matched.\n",
        "\n",
        "ðŸ“Œ Applications\n",
        "\n",
        "Duplicate review detection\n",
        "\n",
        "Sentiment alignment\n",
        "\n",
        "Chatbot and FAQ matching\n",
        "\n",
        "Plagiarism detection in review systems"
      ],
      "metadata": {
        "id": "aNIP-Gik0r_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJg4jsSNTAuw",
        "outputId": "fba26f6b-abb2-4612-c289-9634aec3ed98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-text) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-text) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (80.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (2.14.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub>=0.13.0->tensorflow-text) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.40.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY-z0DMOTiU6",
        "outputId": "e1f28dd9-6a92-43d4-ee13-76654439ce66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.14.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.11/dist-packages (from unstructured) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.24.3)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.13.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.34.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (2024.11.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.11/dist-packages (from python-oxmsg->unstructured) (0.47)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2025.4.26)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (2.11.4)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (5.5.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (0.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Collecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: packaging\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tf-keras~=2.17, but you have tf-keras 2.15.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, but you have tf-keras 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed packaging-24.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import sys\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hHHuWTpTpWk",
        "outputId": "061e3fcd-c78c-4308-bc75-d7c7e6a7422f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_words = set(nltk.corpus.words.words())\n",
        "stop = stopwords.words('english')"
      ],
      "metadata": {
        "id": "_KOO5VxCTttj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_imdb = pd.read_csv('/content/Copy of IMDB Dataset.csv', on_bad_lines='skip', quoting=3)\n",
        "# on_bad_lines='skip' will skip rows with errors\n",
        "# quoting=3 will tell pandas to ignore quotes inside fields enclosed in double quotes\n",
        "\n",
        "df_imdb = df_imdb.sample(frac=1).reset_index(drop=True)\n",
        "# Apply lower() only to string values\n",
        "df_imdb['review'] = df_imdb['review'].apply(lambda x: x.lower() if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "3XoQd0X9TxWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the html strips Helper for main 1\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets Helper for main 1\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text - Main 1\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "# Lemmatize words\n",
        "def my_lemmatizer(x):\n",
        "    lemmatizer= WordNetLemmatizer()\n",
        "    return ' '.join(list(map(lemmatizer.lemmatize, x.split())))\n",
        "\n",
        "### Non English words removal\n",
        "def non_eng_word_removal(x):\n",
        "    return \" \".join(w for w in nltk.wordpunct_tokenize(x) if w.lower() in eng_words or not w.isalpha())"
      ],
      "metadata": {
        "id": "kZsDibiDT5bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_imdb['review'] = df_imdb['review'].astype(str).apply(denoise_text)\n",
        "#Removing stopwords\n",
        "df_imdb['review_stopwords_removed']= df_imdb['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "\n",
        "# Removing punctuations and underscores\n",
        "df_imdb[\"review_stoppunc_removed\"] = df_imdb['review'].str.replace('[^\\w\\s]',' ')\n",
        "df_imdb[\"review_stoppunc_removed\"] = df_imdb['review_stoppunc_removed'].apply(lambda x: x.replace(\"_\",\" \"))\n",
        "\n",
        "\n",
        "### Lemmatizing and removing non eng words\n",
        "df_imdb['review_stoppunc_removed'] = df_imdb['review_stoppunc_removed'].apply(lambda x: my_lemmatizer(x))\n",
        "df_imdb['review_stoppunc_removed'] = df_imdb['review_stoppunc_removed'].apply(lambda x: non_eng_word_removal(x))"
      ],
      "metadata": {
        "id": "nk1rLbkST-aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_imdb['review_stoppunc_removed'].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xp5uTpNUCUp",
        "outputId": "03c85d26-f889-43e7-ac67-dfb30aa93f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45999"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_imdb.columns\n",
        "df_imdb.drop(columns=['review', 'review_stopwords_removed'], inplace = True)"
      ],
      "metadata": {
        "id": "qKnAwbPaUISz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df_imdb['review_stoppunc_removed'].tolist())"
      ],
      "metadata": {
        "id": "BgKUiC2PUNyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(vectorizer.vocabulary_, key = lambda x: x[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU1Mc6YtUQ-k",
        "outputId": "53fb4d45-dbcb-4d3a-c3aa-3837542a4e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['10',\n",
              " '90',\n",
              " '20',\n",
              " '100',\n",
              " '000',\n",
              " '30',\n",
              " '80',\n",
              " '2002',\n",
              " '10rated',\n",
              " '70',\n",
              " '81',\n",
              " '11',\n",
              " '01',\n",
              " '13th',\n",
              " '13',\n",
              " '14',\n",
              " 'm4tv',\n",
              " '45',\n",
              " '06',\n",
              " '1980s',\n",
              " '1966',\n",
              " '1982',\n",
              " '1990',\n",
              " '1950s',\n",
              " 'nan',\n",
              " 'ha',\n",
              " 'watching',\n",
              " 'can',\n",
              " 'safely',\n",
              " 'waste',\n",
              " 'cab',\n",
              " 'fall',\n",
              " 'fantasy',\n",
              " 'eagerly',\n",
              " 'man',\n",
              " 'lay',\n",
              " 'baker',\n",
              " 'half',\n",
              " 'same',\n",
              " 'canal',\n",
              " 'van',\n",
              " 'happy',\n",
              " 'bad',\n",
              " 'part',\n",
              " 'make',\n",
              " 'eager',\n",
              " 'day',\n",
              " 'fact',\n",
              " 'may',\n",
              " 'have',\n",
              " 'major',\n",
              " 'sad',\n",
              " 'landscape',\n",
              " 'hardly',\n",
              " 'many',\n",
              " 'sadly',\n",
              " 'maybe',\n",
              " 'fan',\n",
              " 'cannot',\n",
              " 'wa',\n",
              " 'saw',\n",
              " 'making',\n",
              " 'mani',\n",
              " 'way',\n",
              " 'want',\n",
              " 'rating',\n",
              " 'say',\n",
              " 'cavalcade',\n",
              " 'watchable',\n",
              " 'game',\n",
              " 'havoc',\n",
              " 'various',\n",
              " 'watch',\n",
              " 'racism',\n",
              " 'garbage',\n",
              " 'satisfy',\n",
              " 'save',\n",
              " 'lack',\n",
              " 'fascinating',\n",
              " 'pantheon',\n",
              " 'saint',\n",
              " 'had',\n",
              " 'family',\n",
              " 'rather',\n",
              " 'massive',\n",
              " 'tang',\n",
              " 'made',\n",
              " 'matter',\n",
              " 'saying',\n",
              " 'fairly',\n",
              " 'palm',\n",
              " 'dandy',\n",
              " 'jane',\n",
              " 'take',\n",
              " 'camera',\n",
              " 'male',\n",
              " 'mantis',\n",
              " 'past',\n",
              " 'pain',\n",
              " 'taint',\n",
              " 'raccoon',\n",
              " 'rain',\n",
              " 'map',\n",
              " 'call',\n",
              " 'each',\n",
              " 'painful',\n",
              " 'catch',\n",
              " 'pap',\n",
              " 'mass',\n",
              " 'badly',\n",
              " 'said',\n",
              " 'sample',\n",
              " 'ball',\n",
              " 'wall',\n",
              " 'latter',\n",
              " 'basic',\n",
              " 'casting',\n",
              " 'based',\n",
              " 'rate',\n",
              " 'fascinatingly',\n",
              " 'hatred',\n",
              " 'factor',\n",
              " 'rated',\n",
              " 'earth',\n",
              " 'saucer',\n",
              " 'hack',\n",
              " 'machine',\n",
              " 'candidate',\n",
              " 'masterpiece',\n",
              " 'care',\n",
              " 'lame',\n",
              " 'talk',\n",
              " 'cast',\n",
              " 'raised',\n",
              " 'massacre',\n",
              " 'taken',\n",
              " 'particularly',\n",
              " 'radio',\n",
              " 'manage',\n",
              " 'hank',\n",
              " 'walk',\n",
              " 'nature',\n",
              " 'dark',\n",
              " 'fare',\n",
              " 'far',\n",
              " 'tale',\n",
              " 'wasted',\n",
              " 'last',\n",
              " 'sang',\n",
              " 'hard',\n",
              " 'laying',\n",
              " 'safe',\n",
              " 'wait',\n",
              " 'easy',\n",
              " 'castle',\n",
              " 'packer',\n",
              " 'range',\n",
              " 'park',\n",
              " 'sadness',\n",
              " 'walking',\n",
              " 'rare',\n",
              " 'wartime',\n",
              " 'later',\n",
              " 'paramount',\n",
              " 'mae',\n",
              " 'hair',\n",
              " 'came',\n",
              " 'jack',\n",
              " 'law',\n",
              " 'easily',\n",
              " 'happening',\n",
              " 'mall',\n",
              " 'hale',\n",
              " 'war',\n",
              " 'hamburger',\n",
              " 'ran',\n",
              " 'back',\n",
              " 'was',\n",
              " 'carried',\n",
              " 'barny',\n",
              " 'native',\n",
              " 'harp',\n",
              " 'caught',\n",
              " 'handy',\n",
              " 'rack',\n",
              " 'landmark',\n",
              " 'maker',\n",
              " 'gay',\n",
              " 'tax',\n",
              " 'hackneyed',\n",
              " 'damn',\n",
              " 'master',\n",
              " 'latest',\n",
              " 'radical',\n",
              " 'late',\n",
              " 'lan',\n",
              " 'wang',\n",
              " 'hart',\n",
              " 'dawn',\n",
              " 'laughing',\n",
              " 'sacrificing',\n",
              " 'hall',\n",
              " 'fame',\n",
              " 'hand',\n",
              " 'hat',\n",
              " 'language',\n",
              " 'watery',\n",
              " 'daring',\n",
              " 'pay',\n",
              " 'mary',\n",
              " 'larry',\n",
              " 'mail',\n",
              " 'daughter',\n",
              " 'absolutely',\n",
              " 'about',\n",
              " 'able',\n",
              " 'absolute',\n",
              " 'obsession',\n",
              " 'obvious',\n",
              " 'abruptly',\n",
              " 'oblivion',\n",
              " 'obviously',\n",
              " 'abysmal',\n",
              " 'oblivious',\n",
              " 'above',\n",
              " 'science',\n",
              " 'accident',\n",
              " 'accomplished',\n",
              " 'schneider',\n",
              " 'script',\n",
              " 'actor',\n",
              " 'screenwriter',\n",
              " 'scene',\n",
              " 'score',\n",
              " 'acting',\n",
              " 'achievement',\n",
              " 'scale',\n",
              " 'action',\n",
              " 'actually',\n",
              " 'screen',\n",
              " 'scary',\n",
              " 'scope',\n",
              " 'actress',\n",
              " 'idea',\n",
              " 'admirably',\n",
              " 'advance',\n",
              " 'advised',\n",
              " 'add',\n",
              " 'educational',\n",
              " 'adventure',\n",
              " 'admitted',\n",
              " 'adopted',\n",
              " 'delete',\n",
              " 'negative',\n",
              " 'benefit',\n",
              " 'well',\n",
              " 'textbook',\n",
              " 'legacy',\n",
              " 'because',\n",
              " 'be',\n",
              " 'we',\n",
              " 'declare',\n",
              " 'feeling',\n",
              " 'few',\n",
              " 'get',\n",
              " 'never',\n",
              " 'level',\n",
              " 'better',\n",
              " 'recommend',\n",
              " 'release',\n",
              " 'reliable',\n",
              " 're',\n",
              " 'being',\n",
              " 'regularly',\n",
              " 'her',\n",
              " 'den',\n",
              " 'dempster',\n",
              " 'below',\n",
              " 'decent',\n",
              " 'sense',\n",
              " 'seen',\n",
              " 'learn',\n",
              " 'real',\n",
              " 'meaning',\n",
              " 'next',\n",
              " 'people',\n",
              " 'personality',\n",
              " 'reality',\n",
              " 'perception',\n",
              " 'neat',\n",
              " 'despite',\n",
              " 'regime',\n",
              " 'representation',\n",
              " 'become',\n",
              " 'perhaps',\n",
              " 'best',\n",
              " 'really',\n",
              " 'regular',\n",
              " 'renter',\n",
              " 'regret',\n",
              " 'fetch',\n",
              " 'sell',\n",
              " 'beautiful',\n",
              " 'help',\n",
              " 'respect',\n",
              " 'lesson',\n",
              " 'very',\n",
              " 'teller',\n",
              " 'remember',\n",
              " 'resulting',\n",
              " 'me',\n",
              " 'rejoice',\n",
              " 'new',\n",
              " 'rest',\n",
              " 'season',\n",
              " 'seeing',\n",
              " 'beneath',\n",
              " 'left',\n",
              " 'series',\n",
              " 'were',\n",
              " 'let',\n",
              " 'menace',\n",
              " 'year',\n",
              " 'resume',\n",
              " 'depth',\n",
              " 'genie',\n",
              " 'version',\n",
              " 'generous',\n",
              " 'less',\n",
              " 'near',\n",
              " 'see',\n",
              " 'hell',\n",
              " 'here',\n",
              " 'terrorism',\n",
              " 'megalomania',\n",
              " 'feudal',\n",
              " 'self',\n",
              " 'serving',\n",
              " 'seeking',\n",
              " 'set',\n",
              " 'bet',\n",
              " 'terror',\n",
              " 'beyond',\n",
              " 'revelation',\n",
              " 'perfect',\n",
              " 'definitely',\n",
              " 'deeply',\n",
              " 'he',\n",
              " 'certain',\n",
              " 'read',\n",
              " 'decided',\n",
              " 'recommendation',\n",
              " 'least',\n",
              " 'keep',\n",
              " 'several',\n",
              " 'head',\n",
              " 'memory',\n",
              " 'beast',\n",
              " 'verdict',\n",
              " 'relief',\n",
              " 'need',\n",
              " 'getting',\n",
              " 'feel',\n",
              " 'medium',\n",
              " 'hearts',\n",
              " 'neither',\n",
              " 'meditative',\n",
              " 'central',\n",
              " 'mellow',\n",
              " 'seriously',\n",
              " 'rely',\n",
              " 'yes',\n",
              " 'second',\n",
              " 'deserve',\n",
              " 'depiction',\n",
              " 'dead',\n",
              " 'merit',\n",
              " 'before',\n",
              " 'mention',\n",
              " 'delirium',\n",
              " 'yeah',\n",
              " 'secondly',\n",
              " 'peace',\n",
              " 'death',\n",
              " 'beth',\n",
              " 'beginning',\n",
              " 'beauty',\n",
              " 'heart',\n",
              " 'reason',\n",
              " 'sequence',\n",
              " 'period',\n",
              " 'seem',\n",
              " 'nelson',\n",
              " 'desperately',\n",
              " 'welcome',\n",
              " 'leave',\n",
              " 'rental',\n",
              " 'secret',\n",
              " 'believable',\n",
              " 'healing',\n",
              " 'remake',\n",
              " 'ready',\n",
              " 'relationship',\n",
              " 'yet',\n",
              " 'debut',\n",
              " 'reviewer',\n",
              " 'went',\n",
              " 'legend',\n",
              " 'zero',\n",
              " 'between',\n",
              " 'mess',\n",
              " 'bend',\n",
              " 'ten',\n",
              " 'message',\n",
              " 'heyday',\n",
              " 'west',\n",
              " 'sent',\n",
              " 'terrible',\n",
              " 'deserving',\n",
              " 'penis',\n",
              " 'feat',\n",
              " 'certainly',\n",
              " 'deliciously',\n",
              " 'mean',\n",
              " 'rent',\n",
              " 'sequel',\n",
              " 'terminator',\n",
              " 'desperate',\n",
              " 'detective',\n",
              " 'beat',\n",
              " 'jerk',\n",
              " 'genre',\n",
              " 'remains',\n",
              " 'league',\n",
              " 'deep',\n",
              " 'red',\n",
              " 'received',\n",
              " 'believe',\n",
              " 'feature',\n",
              " 'remotely',\n",
              " 'perplexing',\n",
              " 'reaction',\n",
              " 'been',\n",
              " 'technically',\n",
              " 'herself',\n",
              " 'meat',\n",
              " 'realizing',\n",
              " 'lead',\n",
              " 'healthy',\n",
              " 'deal',\n",
              " 'meet',\n",
              " 'development',\n",
              " 'refreshing',\n",
              " 'sexuality',\n",
              " 'realism',\n",
              " 'letter',\n",
              " 'television',\n",
              " 'repeat',\n",
              " 'felt',\n",
              " 'ret',\n",
              " 'terry',\n",
              " 'generally',\n",
              " 'heroine',\n",
              " 'refreshingly',\n",
              " 'of',\n",
              " 'offend',\n",
              " 'off',\n",
              " 'after',\n",
              " 'effectively',\n",
              " 'often',\n",
              " 'effort',\n",
              " 'if',\n",
              " 'affair',\n",
              " 'afraid',\n",
              " 'offensive',\n",
              " 'age',\n",
              " 'ago',\n",
              " 'again',\n",
              " 'agent',\n",
              " 'shame',\n",
              " 'this',\n",
              " 'the',\n",
              " 'they',\n",
              " 'thats',\n",
              " 'that',\n",
              " 'who',\n",
              " 'should',\n",
              " 'there',\n",
              " 'thing',\n",
              " 'therefore',\n",
              " 'chance',\n",
              " 'change',\n",
              " 'these',\n",
              " 'through',\n",
              " 'what',\n",
              " 'thread',\n",
              " 'whole',\n",
              " 'thus',\n",
              " 'shaped',\n",
              " 'while',\n",
              " 'thought',\n",
              " 'when',\n",
              " 'than',\n",
              " 'chuckle',\n",
              " 'then',\n",
              " 'theme',\n",
              " 'which',\n",
              " 'show',\n",
              " 'shot',\n",
              " 'them',\n",
              " 'whiny',\n",
              " 'their',\n",
              " 'shakti',\n",
              " 'china',\n",
              " 'whose',\n",
              " 'think',\n",
              " 'though',\n",
              " 'thousand',\n",
              " 'thumb',\n",
              " 'cheap',\n",
              " 'charge',\n",
              " 'choppy',\n",
              " 'where',\n",
              " 'character',\n",
              " 'themselves',\n",
              " 'three',\n",
              " 'chapeau',\n",
              " 'ghost',\n",
              " 'thoroughly',\n",
              " 'thank',\n",
              " 'whatever',\n",
              " 'cheerleader',\n",
              " 'throughout',\n",
              " 'whether',\n",
              " 'oh',\n",
              " 'showing',\n",
              " 'chiller',\n",
              " 'those',\n",
              " 'shown',\n",
              " 'thanks',\n",
              " 'whoever',\n",
              " 'threaten',\n",
              " 'chips',\n",
              " 'chest',\n",
              " 'chair',\n",
              " 'why',\n",
              " 'shopping',\n",
              " 'check',\n",
              " 'chunk',\n",
              " 'she',\n",
              " 'ship',\n",
              " 'choir',\n",
              " 'chicken',\n",
              " 'thirty',\n",
              " 'theatrical',\n",
              " 'photography',\n",
              " 'chatting',\n",
              " 'wholesome',\n",
              " 'shocker',\n",
              " 'whenever',\n",
              " 'cheese',\n",
              " 'charming',\n",
              " 'shelly',\n",
              " 'film',\n",
              " 'mind',\n",
              " 'his',\n",
              " 'missing',\n",
              " 'might',\n",
              " 'fight',\n",
              " 'time',\n",
              " 'rise',\n",
              " 'like',\n",
              " 'fiction',\n",
              " 'giant',\n",
              " 'hit',\n",
              " 'girl',\n",
              " 'military',\n",
              " 'liturgy',\n",
              " 'with',\n",
              " 'fitting',\n",
              " 'disintegrate',\n",
              " 'virtuality',\n",
              " 'history',\n",
              " 'diamond',\n",
              " 'micro',\n",
              " 'life',\n",
              " 'since',\n",
              " 'will',\n",
              " 'give',\n",
              " 'first',\n",
              " 'dig',\n",
              " 'fi',\n",
              " 'right',\n",
              " 'find',\n",
              " 'circumstance',\n",
              " 'line',\n",
              " 'himself',\n",
              " 'him',\n",
              " 'bizarre',\n",
              " 'wing',\n",
              " 'situation',\n",
              " 'video',\n",
              " 'director',\n",
              " 'given',\n",
              " 'bit',\n",
              " 'view',\n",
              " 'simply',\n",
              " 'figure',\n",
              " 'different',\n",
              " 'kind',\n",
              " 'living',\n",
              " 'fine',\n",
              " 'willing',\n",
              " 'did',\n",
              " 'wizard',\n",
              " 'highly',\n",
              " 'minute',\n",
              " 'without',\n",
              " 'mindless',\n",
              " 'literally',\n",
              " 'window',\n",
              " 'miss',\n",
              " 'air',\n",
              " 'either',\n",
              " 'simple',\n",
              " 'minded',\n",
              " 'discern',\n",
              " 'silver',\n",
              " 'similar',\n",
              " 'direction',\n",
              " 'directed',\n",
              " 'ring',\n",
              " 'fiend',\n",
              " 'dismal',\n",
              " 'piece',\n",
              " 'titanic',\n",
              " 'division',\n",
              " 'night',\n",
              " 'little',\n",
              " 'title',\n",
              " 'riveting',\n",
              " 'cinematic',\n",
              " 'wild',\n",
              " 'dialogue',\n",
              " 'big',\n",
              " 'cinema',\n",
              " 'lighting',\n",
              " 'firstly',\n",
              " 'five',\n",
              " 'kill',\n",
              " 'hint',\n",
              " 'side',\n",
              " 'aid',\n",
              " 'picture',\n",
              " 'lifelong',\n",
              " 'nick',\n",
              " 'milliard',\n",
              " 'disappointment',\n",
              " 'sincere',\n",
              " 'displayed',\n",
              " 'disconnected',\n",
              " 'midnight',\n",
              " 'sign',\n",
              " 'die',\n",
              " 'kidnap',\n",
              " 'risk',\n",
              " 'distance',\n",
              " 'times',\n",
              " 'direct',\n",
              " 'timeless',\n",
              " 'fix',\n",
              " 'disappoint',\n",
              " 'nifty',\n",
              " 'live',\n",
              " 'list',\n",
              " 'difference',\n",
              " 'kick',\n",
              " 'nice',\n",
              " 'timely',\n",
              " 'mistake',\n",
              " 'titter',\n",
              " 'wish',\n",
              " 'final',\n",
              " 'mildly',\n",
              " 'pitch',\n",
              " 'dinosaur',\n",
              " 'minimal',\n",
              " 'pick',\n",
              " 'circle',\n",
              " 'sit',\n",
              " 'bird',\n",
              " 'aim',\n",
              " 'distracted',\n",
              " 'limited',\n",
              " 'wind',\n",
              " 'pimping',\n",
              " 'singing',\n",
              " 'gimmick',\n",
              " 'disaster',\n",
              " 'dirt',\n",
              " 'gift',\n",
              " 'diversion',\n",
              " 'pilot',\n",
              " 'violence',\n",
              " 'viewer',\n",
              " 'bill',\n",
              " 'miserable',\n",
              " 'skater',\n",
              " 'sky',\n",
              " 'all',\n",
              " 'also',\n",
              " 'old',\n",
              " 'plane',\n",
              " 'play',\n",
              " 'clear',\n",
              " 'plot',\n",
              " 'elsewhere',\n",
              " 'although',\n",
              " 'elegance',\n",
              " 'else',\n",
              " 'glad',\n",
              " 'always',\n",
              " 'climactic',\n",
              " 'place',\n",
              " 'slasher',\n",
              " 'classic',\n",
              " 'clumsily',\n",
              " 'clasping',\n",
              " 'plain',\n",
              " 'flick',\n",
              " 'illuminated',\n",
              " 'flack',\n",
              " 'clip',\n",
              " 'flying',\n",
              " 'classical',\n",
              " 'altogether',\n",
              " 'plenty',\n",
              " 'alone',\n",
              " 'sleeping',\n",
              " 'alan',\n",
              " 'plan',\n",
              " 'cleaning',\n",
              " 'electric',\n",
              " 'almost',\n",
              " 'alien',\n",
              " 'black',\n",
              " 'plus',\n",
              " 'ultimate',\n",
              " 'fling',\n",
              " 'ultimately',\n",
              " 'plumage',\n",
              " 'plausible',\n",
              " 'closer',\n",
              " 'please',\n",
              " 'floor',\n",
              " 'elegant',\n",
              " 'elder',\n",
              " 'already',\n",
              " 'slight',\n",
              " 'flat',\n",
              " 'empire',\n",
              " 'am',\n",
              " 'amazing',\n",
              " 'small',\n",
              " 'amazed',\n",
              " 'embarrass',\n",
              " 'importance',\n",
              " 'emphatically',\n",
              " 'impressive',\n",
              " 'among',\n",
              " 'emotion',\n",
              " 'imagination',\n",
              " 'impossible',\n",
              " 'amateurish',\n",
              " 'amiss',\n",
              " 'smile',\n",
              " 'em',\n",
              " 'smell',\n",
              " 'smith',\n",
              " 'and',\n",
              " 'on',\n",
              " 'one',\n",
              " 'ancient',\n",
              " 'an',\n",
              " 'intuition',\n",
              " 'enough',\n",
              " 'any',\n",
              " 'animal',\n",
              " 'unless',\n",
              " 'in',\n",
              " 'anyone',\n",
              " 'engaged',\n",
              " 'intention',\n",
              " 'end',\n",
              " 'only',\n",
              " 'unreal',\n",
              " 'enjoyable',\n",
              " 'unintended',\n",
              " 'entertaining',\n",
              " 'intellectual',\n",
              " 'once',\n",
              " 'into',\n",
              " 'unfair',\n",
              " 'entire',\n",
              " 'industry',\n",
              " 'entertainment',\n",
              " 'interweaving',\n",
              " 'university',\n",
              " 'en',\n",
              " 'know',\n",
              " 'entirely',\n",
              " 'another',\n",
              " 'ending',\n",
              " 'anti',\n",
              " 'anything',\n",
              " 'intensity',\n",
              " 'intelligent',\n",
              " 'enjoy',\n",
              " 'independent',\n",
              " 'enjoyment',\n",
              " 'instead',\n",
              " 'interested',\n",
              " 'ended',\n",
              " 'indeed',\n",
              " 'ingenious',\n",
              " 'enlighten',\n",
              " 'unattractive',\n",
              " 'anyway',\n",
              " 'under',\n",
              " 'antichrist',\n",
              " 'unusual',\n",
              " 'interesting',\n",
              " 'unconquerable',\n",
              " 'interrogation',\n",
              " 'uncontrollable',\n",
              " 'incredibly',\n",
              " 'until',\n",
              " 'answer',\n",
              " 'anxiously',\n",
              " 'entry',\n",
              " 'innocent',\n",
              " 'knowing',\n",
              " 'environmental',\n",
              " 'enter',\n",
              " 'entertainingly',\n",
              " 'enemy',\n",
              " 'intriguing',\n",
              " 'interest',\n",
              " 'anonymous',\n",
              " 'unlikely',\n",
              " 'unused',\n",
              " 'inappropriate',\n",
              " 'unique',\n",
              " 'enlightening',\n",
              " 'known',\n",
              " 'angrily',\n",
              " 'inspiration',\n",
              " 'intended',\n",
              " 'intricate',\n",
              " 'uncomfortable',\n",
              " 'your',\n",
              " 'positive',\n",
              " 'worth',\n",
              " 'not',\n",
              " 'political',\n",
              " 'you',\n",
              " 'someone',\n",
              " 'boring',\n",
              " 'movie',\n",
              " 'no',\n",
              " 'don',\n",
              " 'would',\n",
              " 'joe',\n",
              " 'to',\n",
              " 'most',\n",
              " 'noise',\n",
              " 'for',\n",
              " 'volume',\n",
              " 'moment',\n",
              " 'voice',\n",
              " 'sound',\n",
              " 'hold',\n",
              " 'horrible',\n",
              " 'gore',\n",
              " 'come',\n",
              " 'social',\n",
              " 'losing',\n",
              " 'follow',\n",
              " 'could',\n",
              " 'sometimes',\n",
              " 'some',\n",
              " 'good',\n",
              " 'doing',\n",
              " 'so',\n",
              " 'soviet',\n",
              " 'modern',\n",
              " 'force',\n",
              " 'rough',\n",
              " 'horror',\n",
              " 'more',\n",
              " 'comedian',\n",
              " 'goer',\n",
              " 'now',\n",
              " 'money',\n",
              " 'go',\n",
              " 'do',\n",
              " 'lose',\n",
              " 'work',\n",
              " 'got',\n",
              " 'hollow',\n",
              " 'hole',\n",
              " 'something',\n",
              " 'going',\n",
              " 'hope',\n",
              " 'love',\n",
              " 'how',\n",
              " 'role',\n",
              " 'totally',\n",
              " 'point',\n",
              " 'politics',\n",
              " 'condemn',\n",
              " 'total',\n",
              " 'local',\n",
              " 'normal',\n",
              " 'worthy',\n",
              " 'nominated',\n",
              " 'condition',\n",
              " 'worse',\n",
              " 'look',\n",
              " 'couple',\n",
              " 'hour',\n",
              " 'possible',\n",
              " 'course',\n",
              " 'focus',\n",
              " 'royal',\n",
              " 'poverty',\n",
              " 'power',\n",
              " 'notable',\n",
              " 'hot',\n",
              " 'noir',\n",
              " 'nowhere',\n",
              " 'lot',\n",
              " 'wow',\n",
              " 'gotten',\n",
              " 'ponderous',\n",
              " 'former',\n",
              " 'too',\n",
              " 'footage',\n",
              " 'book',\n",
              " 'goal',\n",
              " 'wonderful',\n",
              " 'yourself',\n",
              " 'youth',\n",
              " 'vomiting',\n",
              " 'comic',\n",
              " 'dose',\n",
              " 'copy',\n",
              " 'consumption',\n",
              " 'nobody',\n",
              " 'young',\n",
              " 'woman',\n",
              " 'complete',\n",
              " 'notably',\n",
              " 'lord',\n",
              " 'commendable',\n",
              " 'motion',\n",
              " 'nor',\n",
              " 'compare',\n",
              " 'poor',\n",
              " 'hostel',\n",
              " 'notice',\n",
              " 'monument',\n",
              " 'job',\n",
              " 'worst',\n",
              " 'worked',\n",
              " 'word',\n",
              " 'hoop',\n",
              " 'song',\n",
              " 'down',\n",
              " 'looking',\n",
              " 'coming',\n",
              " 'commercial',\n",
              " 'gone',\n",
              " 'round',\n",
              " 'confrontation',\n",
              " 'both',\n",
              " 'god',\n",
              " 'soul',\n",
              " 'nothing',\n",
              " 'won',\n",
              " 'torture',\n",
              " 'concept',\n",
              " 'told',\n",
              " 'house',\n",
              " 'pointless',\n",
              " 'boost',\n",
              " 'body',\n",
              " 'count',\n",
              " 'nonsense',\n",
              " 'top',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U tensorflow==2.14.0 tensorflow-text==2.14.0 tensorflow-hub==0.15.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "11-IzBWJU3Sb",
        "outputId": "8d529759-be9a-4e5a-bea4-3568425aa6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.14.0 in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow-text==2.14.0 in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
            "Collecting tensorflow-hub==0.15.0\n",
            "  Using cached tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (80.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.40.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n",
            "Using cached tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\n",
            "Installing collected packages: tensorflow-hub\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.16.1\n",
            "    Uninstalling tensorflow-hub-0.16.1:\n",
            "      Successfully uninstalled tensorflow-hub-0.16.1\n",
            "Successfully installed tensorflow-hub-0.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow_hub"
                ]
              },
              "id": "0dead96dcb98455a9263525bcd7902dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy<2.0 --force-reinstall\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7DfGy7hXTFr",
        "outputId": "1d0ee32b-f7e1-493b-f5d0-f67ddbec825f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 2.0: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy<2.0 tensorflow==2.14 tensorflow_hub tensorflow_text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmpL-ZivXltZ",
        "outputId": "0d2437b6-fc2c-47fa-dff3-2b76d9f47e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 2.0: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow==2.14.0 tensorflow-text==2.14.0 tensorflow-hub==0.15.0 numpy==1.23.5 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlM_jX1OYeXB",
        "outputId": "c2d8e473-77f0-41c4-937a-54293ca923c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.14.0\n",
            "  Using cached tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-text==2.14.0\n",
            "  Using cached tensorflow_text-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-hub==0.15.0\n",
            "  Using cached tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow==2.14.0)\n",
            "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow==2.14.0)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow==2.14.0)\n",
            "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.14.0)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow==2.14.0)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py>=2.9.0 (from tensorflow==2.14.0)\n",
            "  Using cached h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow==2.14.0)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U numpy==1.23.5 --force-reinstall\n",
        "!pip install -U tensorflow==2.14.0 tensorflow-text==2.14.0 tensorflow-hub==0.15.0 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzeznvogY_e4",
        "outputId": "24998bba-91bb-4672-a518-a31b721acc0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U jaxlib jax==0.4.14 numpy==1.23.5 --force-reinstall\n",
        "!pip install -U tensorflow==2.14.0 tensorflow-text==2.14.0 tensorflow-hub==0.15.0 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tZ6Kn0l9ZUVq",
        "outputId": "21a90be2-6b6d-48b7-cbd6-b97d57f03826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jaxlib\n",
            "  Using cached jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax==0.4.14\n",
            "  Using cached jax-0.4.14-py3-none-any.whl\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting ml_dtypes>=0.2.0 (from jax==0.4.14)\n",
            "  Using cached ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting opt_einsum (from jax==0.4.14)\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting scipy>=1.7 (from jax==0.4.14)\n",
            "  Using cached scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "INFO: pip is looking at multiple versions of jaxlib to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jaxlib\n",
            "  Using cached jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "  Using cached jaxlib-0.5.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "  Using cached jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "  Using cached jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Using cached jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Using cached jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "  Using cached jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: pip is still looking at multiple versions of jaxlib to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "  Using cached jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "  Using cached jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Using cached jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "Using cached ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "Using cached scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Installing collected packages: opt_einsum, numpy, scipy, ml_dtypes, jaxlib, jax\n",
            "  Attempting uninstall: opt_einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: ml_dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.14.0 requires ml-dtypes==0.2.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.3.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.14 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tf-keras~=2.17, but you have tf-keras 2.15.0 which is incompatible.\n",
            "optax 0.2.4 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "yfinance 0.2.59 requires protobuf<6,>=5.29.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.14 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, but you have tf-keras 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.4.14 jaxlib-0.4.30 ml_dtypes-0.5.1 numpy-1.23.5 opt_einsum-3.4.0 scipy-1.15.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "d415cfb78223456abf675eff00a53674"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.14.0\n",
            "  Using cached tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-text==2.14.0\n",
            "  Using cached tensorflow_text-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-hub==0.15.0\n",
            "  Using cached tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow==2.14.0)\n",
            "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow==2.14.0)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow==2.14.0)\n",
            "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.14.0)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow==2.14.0)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py>=2.9.0 (from tensorflow==2.14.0)\n",
            "  Using cached h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow==2.14.0)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow==2.14.0)\n",
            "  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy>=1.23.5 (from tensorflow==2.14.0)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow==2.14.0)\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting packaging (from tensorflow==2.14.0)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y jaxlib jax numpy\n",
        "!pip install -U numpy==1.24.3  # TensorFlow 2.14 officially supports NumPy 1.24.3\n",
        "!pip install -U tensorflow==2.14.0 tensorflow-text==2.14.0 tensorflow-hub==0.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qjPCadC0aNSl",
        "outputId": "45d22e48-207f-4926-a7da-f7492b1e05ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: jaxlib 0.4.30\n",
            "Uninstalling jaxlib-0.4.30:\n",
            "  Successfully uninstalled jaxlib-0.4.30\n",
            "Found existing installation: jax 0.4.14\n",
            "Uninstalling jax-0.4.14:\n",
            "  Successfully uninstalled jax-0.4.14\n",
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "Collecting numpy==1.24.3\n",
            "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.89 requires jax>=0.4.27, which is not installed.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, which is not installed.\n",
            "flax 0.10.6 requires jax>=0.5.1, which is not installed.\n",
            "optax 0.2.4 requires jax>=0.4.27, which is not installed.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, which is not installed.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
            "tensorflow 2.14.0 requires ml-dtypes==0.2.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tf-keras~=2.17, but you have tf-keras 2.15.0 which is incompatible.\n",
            "yfinance 0.2.59 requires protobuf<6,>=5.29.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, but you have tf-keras 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "269fa2eab1ef4747b05ef14fe7c45be0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.14.0 in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow-text==2.14.0 in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow-hub==0.15.0 in /usr/local/lib/python3.11/dist-packages (0.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (18.1.1)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow==2.14.0)\n",
            "  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (80.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.40.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n",
            "Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Installing collected packages: ml-dtypes\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.1\n",
            "    Uninstalling ml_dtypes-0.5.1:\n",
            "      Successfully uninstalled ml_dtypes-0.5.1\n",
            "\u001b[31m  ERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.24.3 --force-reinstall\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "-f73sVohce-M",
        "outputId": "6e3529f4-980d-4952-b246-38d13cc321f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.3\n",
            "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.89 requires jax>=0.4.27, which is not installed.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, which is not installed.\n",
            "flax 0.10.6 requires jax>=0.5.1, which is not installed.\n",
            "optax 0.2.4 requires jax>=0.4.27, which is not installed.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, which is not installed.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tf-keras~=2.17, but you have tf-keras 2.15.0 which is incompatible.\n",
            "yfinance 0.2.59 requires protobuf<6,>=5.29.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, but you have tf-keras 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "a29884080f324a35bddfc47a75a84930"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.14.0 tensorflow-hub tensorflow-text numpy==1.24.3 ml-dtypes==0.2.0 protobuf==3.20.* --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jHxOQr-yclKg",
        "outputId": "d7c96795-5521-412f-ffcb-007e6bcb57dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.14.0\n",
            "  Using cached tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-hub\n",
            "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-text\n",
            "  Using cached tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting numpy==1.24.3\n",
            "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting ml-dtypes==0.2.0\n",
            "  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting protobuf==3.20.*\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow==2.14.0)\n",
            "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow==2.14.0)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow==2.14.0)\n",
            "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.14.0)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow==2.14.0)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py>=2.9.0 (from tensorflow==2.14.0)\n",
            "  Using cached h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow==2.14.0)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow==2.14.0)\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting packaging (from tensorflow==2.14.0)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting setuptools (from tensorflow==2.14.0)\n",
            "  Using cached setuptools-80.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting six>=1.12.0 (from tensorflow==2.14.0)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow==2.14.0)\n",
            "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typing-extensions>=3.6.6 (from tensorflow==2.14.0)\n",
            "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.14.0)\n",
            "  Using cached wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.14.0)\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.14.0)\n",
            "  Using cached grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14.0)\n",
            "  Using cached tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.0)\n",
            "  Using cached tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow==2.14.0)\n",
            "  Using cached keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub)\n",
            "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-text\n",
            "  Using cached tensorflow_text-2.18.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "  Using cached tensorflow_text-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "  Using cached tensorflow_text-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "  Using cached tensorflow_text-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "  Using cached tensorflow_text-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "  Using cached tensorflow_text-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.14.0)\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting requests<3,>=2.21.0 (from tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub)\n",
            "  Using cached tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Using cached tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Using cached tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
            "Using cached tensorflow_text-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "Using cached absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "Using cached h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "Using cached keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "Using cached setuptools-80.4.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Using cached wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Using cached google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
            "Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Using cached charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, tf-keras, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, pyasn1, protobuf, packaging, opt-einsum, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, tensorflow-hub, rsa, requests, pyasn1-modules, ml-dtypes, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: libclang\n",
            "    Found existing installation: libclang 18.1.1\n",
            "    Uninstalling libclang-18.1.1:\n",
            "      Successfully uninstalled libclang-18.1.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 25.2.10\n",
            "    Uninstalling flatbuffers-25.2.10:\n",
            "      Successfully uninstalled flatbuffers-25.2.10\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.45.1\n",
            "    Uninstalling wheel-0.45.1:\n",
            "      Successfully uninstalled wheel-0.45.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf-keras 2.15.0\n",
            "    Uninstalling tf-keras-2.15.0:\n",
            "      Successfully uninstalled tf-keras-2.15.0\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 3.1.0\n",
            "    Uninstalling termcolor-3.1.0:\n",
            "      Successfully uninstalled termcolor-3.1.0\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.37.1\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.37.1:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 80.4.0\n",
            "    Uninstalling setuptools-80.4.0:\n",
            "      Successfully uninstalled setuptools-80.4.0\n",
            "  Attempting uninstall: pyasn1\n",
            "    Found existing installation: pyasn1 0.6.1\n",
            "    Uninstalling pyasn1-0.6.1:\n",
            "      Successfully uninstalled pyasn1-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "  Attempting uninstall: oauthlib\n",
            "    Found existing installation: oauthlib 3.2.2\n",
            "    Uninstalling oauthlib-3.2.2:\n",
            "      Successfully uninstalled oauthlib-3.2.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.8\n",
            "    Uninstalling Markdown-3.8:\n",
            "      Successfully uninstalled Markdown-3.8\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.71.0\n",
            "    Uninstalling grpcio-1.71.0:\n",
            "      Successfully uninstalled grpcio-1.71.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.2\n",
            "    Uninstalling charset-normalizer-3.4.2:\n",
            "      Successfully uninstalled charset-normalizer-3.4.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.5.2\n",
            "    Uninstalling cachetools-5.5.2:\n",
            "      Successfully uninstalled cachetools-5.5.2\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 2.2.2\n",
            "    Uninstalling absl-py-2.2.2:\n",
            "      Successfully uninstalled absl-py-2.2.2\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.15.0\n",
            "    Uninstalling tensorflow-hub-0.15.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.15.0\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9.1\n",
            "    Uninstalling rsa-4.9.1:\n",
            "      Successfully uninstalled rsa-4.9.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1_modules 0.4.2\n",
            "    Uninstalling pyasn1_modules-0.4.2:\n",
            "      Successfully uninstalled pyasn1_modules-0.4.2\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.13.0\n",
            "    Uninstalling h5py-3.13.0:\n",
            "      Successfully uninstalled h5py-3.13.0\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: astunparse\n",
            "    Found existing installation: astunparse 1.6.3\n",
            "    Uninstalling astunparse-1.6.3:\n",
            "      Successfully uninstalled astunparse-1.6.3\n",
            "  Attempting uninstall: requests-oauthlib\n",
            "    Found existing installation: requests-oauthlib 2.0.0\n",
            "    Uninstalling requests-oauthlib-2.0.0:\n",
            "      Successfully uninstalled requests-oauthlib-2.0.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.40.1\n",
            "    Uninstalling google-auth-2.40.1:\n",
            "      Successfully uninstalled google-auth-2.40.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "  Attempting uninstall: tensorflow-text\n",
            "    Found existing installation: tensorflow-text 2.14.0\n",
            "    Uninstalling tensorflow-text-2.14.0:\n",
            "      Successfully uninstalled tensorflow-text-2.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "chex 0.1.89 requires jax>=0.4.27, which is not installed.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, which is not installed.\n",
            "flax 0.10.6 requires jax>=0.5.1, which is not installed.\n",
            "optax 0.2.4 requires jax>=0.4.27, which is not installed.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, which is not installed.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
            "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "langchain-core 0.3.59 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tf-keras~=2.17, but you have tf-keras 2.15.0 which is incompatible.\n",
            "yfinance 0.2.59 requires protobuf<6,>=5.29.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, but you have tf-keras 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 absl-py-2.2.2 astunparse-1.6.3 cachetools-5.5.2 certifi-2025.4.26 charset-normalizer-3.4.2 flatbuffers-25.2.10 gast-0.6.0 google-auth-2.40.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 idna-3.10 keras-2.14.0 libclang-18.1.1 markdown-3.8 ml-dtypes-0.2.0 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.4.0 packaging-25.0 protobuf-3.20.3 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.3 requests-oauthlib-2.0.0 rsa-4.9.1 setuptools-80.4.0 six-1.17.0 tensorboard-2.14.1 tensorboard-data-server-0.7.2 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-hub-0.16.1 tensorflow-io-gcs-filesystem-0.37.1 tensorflow-text-2.14.0 termcolor-3.1.0 tf-keras-2.15.0 typing-extensions-4.13.2 urllib3-2.4.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "certifi",
                  "google",
                  "numpy",
                  "pkg_resources",
                  "six"
                ]
              },
              "id": "458517abb5ca4dcca30c0d6008c266fd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "bert_preprocess_model = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "bert_model =  'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
        "\n",
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(bert_preprocess_model, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(bert_model, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  #net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  #net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "bert_embedding_model= build_classifier_model()"
      ],
      "metadata": {
        "id": "7qBf9mjqdA6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys # Import the sys module\n",
        "bert_embedding_model.summary()\n",
        "print('bert size', sys.getsizeof(bert_embedding_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC-jJlTJdSQ6",
        "outputId": "958a2d33-3ba6-4451-dea2-2ec93c60c2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " text (InputLayer)           [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " preprocessing (KerasLayer)  {'input_mask': (None, 128)   0         ['text[0][0]']                \n",
            "                             , 'input_type_ids': (None,                                           \n",
            "                              128),                                                               \n",
            "                              'input_word_ids': (None,                                            \n",
            "                             128)}                                                                \n",
            "                                                                                                  \n",
            " BERT_encoder (KerasLayer)   {'default': (None, 768),     1094822   ['preprocessing[0][0]',       \n",
            "                              'encoder_outputs': [(None   41         'preprocessing[0][1]',       \n",
            "                             , 128, 768),                            'preprocessing[0][2]']       \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768)],                                                  \n",
            "                              'pooled_output': (None, 7                                           \n",
            "                             68),                                                                 \n",
            "                              'sequence_output': (None,                                           \n",
            "                              128, 768)}                                                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109482241 (417.64 MB)\n",
            "Trainable params: 109482240 (417.64 MB)\n",
            "Non-trainable params: 1 (1.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "bert size 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts= tf.constant([\"This is my name\", \"I am called by this name\"])\n",
        "type(bert_embedding_model.predict(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3MmeIFKdmug",
        "outputId": "f0d1fc6a-11c4-47de-e7fb-6679d38226ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concat_vec= np.hstack(X, bert_embedding_model(tf.constant(df_imdb['review_stoppunc_removed'].tolist())))\n",
        "### Getting error!"
      ],
      "metadata": {
        "id": "-kAZo73bdsIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_records(x_np, start,end):\n",
        "    return x_np[start:end][:]"
      ],
      "metadata": {
        "id": "AVM3ChOHdtnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer # import the TfidfVectorizer class\n",
        "\n",
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "7ziUR4s4i_-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_imdb = pd.read_csv('/content/Copy of IMDB Dataset.csv')\n"
      ],
      "metadata": {
        "id": "38qqu9OTjs3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "eng_words = set(nltk.corpus.words.words())\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# Load the IMDB dataset\n",
        "df_imdb = pd.read_csv('/content/Copy of IMDB Dataset.csv', on_bad_lines='skip', quoting=3)\n",
        "\n",
        "# Apply lower() only to string values\n",
        "df_imdb['review'] = df_imdb['review'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "\n",
        "# Removing the html strips Helper for main 1\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# Removing the square brackets Helper for main 1\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "# Removing the noisy text - Main 1\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "# Lemmatize words\n",
        "def my_lemmatizer(x):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join(list(map(lemmatizer.lemmatize, x.split())))\n",
        "\n",
        "### Non English words removal\n",
        "def non_eng_word_removal(x):\n",
        "    return \" \".join(w for w in nltk.wordpunct_tokenize(x) if w.lower() in eng_words or not w.isalpha())\n",
        "\n",
        "\n",
        "df_imdb['review'] = df_imdb['review'].astype(str).apply(denoise_text)\n",
        "# Removing stopwords\n",
        "df_imdb['review_stopwords_removed'] = df_imdb['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "\n",
        "# Removing punctuations and underscores\n",
        "df_imdb[\"review_stoppunc_removed\"] = df_imdb['review'].str.replace('[^\\w\\s]', ' ')\n",
        "df_imdb[\"review_stoppunc_removed\"] = df_imdb['review_stoppunc_removed'].apply(lambda x: x.replace(\"_\", \" \"))\n",
        "\n",
        "### Lemmatizing and removing non eng words\n",
        "df_imdb['review_stoppunc_removed'] = df_imdb['review_stoppunc_removed'].apply(lambda x: my_lemmatizer(x))\n",
        "df_imdb['review_stoppunc_removed'] = df_imdb['review_stoppunc_removed'].apply(lambda x: non_eng_word_removal(x))\n",
        "\n",
        "# Load a pre-trained BERT model for embedding\n",
        "bert_embedding_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "\n",
        "# Load the preprocessor associated with the BERT model\n",
        "preprocessor = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "\n",
        "# Initialize the TF-IDF vectorizer (if not already done)\n",
        "vectorizer = TfidfVectorizer() # Example, adjust as needed\n",
        "\n",
        "# Fit the vectorizer to your training data\n",
        "# Assuming 'df_imdb' is your DataFrame and 'review_stoppunc_removed' is the text column\n",
        "vectorizer.fit(df_imdb['review_stoppunc_removed'].tolist())  # Fit before using transform\n",
        "\n",
        "batch_size = 10\n",
        "start = 0\n",
        "end = batch_size\n",
        "user_query = \"Hey how are you\"\n",
        "\n",
        "def vectorize_user_query(user_query):\n",
        "    user_query = [user_query]\n",
        "\n",
        "    # 1. Preprocess the user query using the preprocessor\n",
        "    encoder_inputs = preprocessor(user_query)\n",
        "\n",
        "    # 2. Get BERT embeddings using the preprocessed input\n",
        "    bert_vec = bert_embedding_model(encoder_inputs)['pooled_output'].numpy()\n",
        "\n",
        "    # 3. Get TF-IDF vector\n",
        "    tfidf_vec = vectorizer.transform(user_query).toarray()\n",
        "\n",
        "    # 4. Horizontally stack the vectors\n",
        "    return np.hstack((tfidf_vec, bert_vec))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "vectorized_query = vectorize_user_query(user_query)\n",
        "print(vectorized_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXbmqmd4kRcn",
        "outputId": "47bb9501-3c6b-48be-aec2-ae0f825b000f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_text) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (80.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub>=0.13.0->tensorflow_text) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.40.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.2.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.          0.          0.         ... -0.35199001 -0.66522896\n",
            "   0.93488419]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load a pre-trained BERT model for embedding\n",
        "bert_embedding_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "\n",
        "# Load the preprocessor associated with the BERT model\n",
        "preprocessor = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "\n",
        "# Initialize the TF-IDF vectorizer (if not already done)\n",
        "vectorizer = TfidfVectorizer() # Example, adjust as needed\n",
        "\n",
        "# Fit the vectorizer to your training data\n",
        "# Assuming 'df_imdb' is your DataFrame and 'review_stoppunc_removed' is the text column\n",
        "vectorizer.fit(df_imdb['review_stoppunc_removed'].tolist())  # Fit before using transform\n",
        "\n",
        "batch_size = 10\n",
        "start = 0\n",
        "end = batch_size\n",
        "user_query = \"Hey how are you\"\n",
        "\n",
        "def vectorize_user_query(user_query):\n",
        "    user_query = [user_query]\n",
        "\n",
        "    # 1. Preprocess the user query using the preprocessor\n",
        "    encoder_inputs = preprocessor(user_query)\n",
        "\n",
        "    # 2. Get BERT embeddings using the preprocessed input\n",
        "    bert_vec = bert_embedding_model(encoder_inputs)['pooled_output'].numpy()\n",
        "\n",
        "    # 3. Get TF-IDF vector\n",
        "    tfidf_vec = vectorizer.transform(user_query).toarray()\n",
        "\n",
        "    # 4. Horizontally stack the vectors\n",
        "    return np.hstack((tfidf_vec, bert_vec))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "vectorized_query = vectorize_user_query(user_query)\n",
        "print(vectorized_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiAyQPjQjJUn",
        "outputId": "46bd8271-49ae-40a5-cb95-b3fecbab23df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_text) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (80.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub>=0.13.0->tensorflow_text) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.40.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.2.2)\n",
            "[[ 0.          0.          0.         ... -0.35198984 -0.66522896\n",
            "   0.93488413]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gc\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Assuming df_imdb and other necessary variables are already defined\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer and transform the text data\n",
        "# Assuming 'df_imdb' is your DataFrame and 'review_stoppunc_removed' is the text column\n",
        "X = vectorizer.fit_transform(df_imdb['review_stoppunc_removed'].tolist())  # Calculate TF-IDF vectors\n",
        "\n",
        "# Load the preprocessor associated with the BERT model\n",
        "preprocessor = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "\n",
        "\n",
        "# Now you can proceed with your loop\n",
        "df_imdb['similarity'] = 0\n",
        "for i in range(0, X.shape[0] // batch_size + 1):\n",
        "    end = start + batch_size\n",
        "    # Preprocess the text data before passing it to the BERT model\n",
        "    encoder_inputs = preprocessor(df_imdb['review_stoppunc_removed'].iloc[start:end].tolist())\n",
        "    bert_embeddings = bert_embedding_model(encoder_inputs)['pooled_output'].numpy()\n",
        "\n",
        "    concat_vec = np.hstack((X.toarray()[start: end][:], bert_embeddings))\n",
        "    dist = cosine_similarity(concat_vec, vectorize_user_query(user_query))\n",
        "    del concat_vec\n",
        "    print(type(dist))\n",
        "    for i, j in enumerate(range(start, end)):\n",
        "        df_imdb['similarity'].iloc[j] = dist[i]\n",
        "\n",
        "    start = start + batch_size\n",
        "    gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7pPtjCi0lQkq",
        "outputId": "0ed62279-f502-43d0-e345-04d84b93cdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n",
            "<ipython-input-20-d3f9eaf60cf8>:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n",
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.78663386]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n",
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d3f9eaf60cf8>:36: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_imdb['similarity'].iloc[j] = dist[i]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d3f9eaf60cf8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mconcat_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorize_user_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mconcat_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-75d0a915d352>\u001b[0m in \u001b[0;36mvectorize_user_query\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# 2. Get BERT embeddings using the preprocessed input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mbert_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_embedding_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pooled_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# 3. Get TF-IDF vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m       result = smart_cond.smart_cond(training,\n\u001b[0m\u001b[1;32m    251\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                                      lambda: f(training=False))\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     return cond.cond(pred, true_fn=true_fn, false_fn=false_fn,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m       result = smart_cond.smart_cond(training,\n\u001b[1;32m    251\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                                      lambda: f(training=False))\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Unwrap dicts returned by signatures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    874\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    877\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_imdb['similarity']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y3g7HTGImT_6",
        "outputId": "5990f50e-2e03-4def-af89-db53e94d9bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right                                                                                          as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        which set in right from the word GO. Trust me                                                                                                                                                                                                                                                          this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs                                                                                                                                                                          sex or violence. Its is hardcore                                                                                                                                                                                                                               in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City                                                                                                                                                                                        an experimental section of the prison where all the cells have glass fronts and face inwards                                                                                                                                                 so privacy is not high on the agenda. Em City is home to many..Aryans                                                                              Muslims                                                                                   gangstas                                                                                                                                   Latinos                                         Christians                                                                                               Italians                                                                                                            Irish and more....so scuffles                                      death stares          dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences   forget charm                                                           forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal   I couldn't say I was ready for it   but as I watched more   I developed a taste for Oz   and got accustomed to the high levels of graphic violence. Not just violence   but injustice (crooked guards who'll be sold out for a nickel   inmates who'll kill on order and get away with it   well mannered   middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz    0.0\n",
              "\"A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting                                                                    and sometimes discomforting                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"\"has got all the polari\"\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries                           not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which   rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses                                                                                                                                     particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\"                                                                                                                                                                                    positive                                                                                                                                                                                                                                     NaN                                                                                                                                                NaN                                                                                       NaN                                                                                                                                        NaN                                             NaN                                                                                                      NaN                                                                                                                 NaN                                                                NaN                   NaN                                                                                                                                                                                                                                   NaN                                                                    NaN                                                                                                          NaN                                 NaN                     NaN                          NaN                                                                            NaN                                                             NaN                                                 NaN             NaN                                                                                                                            0.0\n",
              "\"I thought this was a wonderful way to spend time on a too hot summer weekend                                                                                                                             sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction                                                                                                     I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson     in this she managed to tone down her \"\"sexy\"\" image and jumped right into a average                                                                                                                                                                            but spirited young woman.<br /><br />This may not be the crown jewel of his career                                                                                                                                                                                                                                                                                  but it was wittier than \"\"Devil Wears Prada\"\" and more interesting than \"\"Superman\"\" a great comedy to go see with friends.\"                                                                                                                positive                                                                                                                                           NaN                                                                                       NaN                                                                                                                                        NaN                                             NaN                                                                                                      NaN                                                                                                                 NaN                                                                NaN                   NaN                                                                                                                                                                                                                                   NaN                                                                    NaN                                                                                                          NaN                                 NaN                     NaN                          NaN                                                                            NaN                                                             NaN                                                 NaN             NaN                                                                                                                            0.0\n",
              "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly    Jake decides to become Rambo and kill the zombie.<br /><br />OK                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie  and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\"                                                                              negative                                                                                                                                                                                                                                                       NaN                                                                                                                                                                                                                                                                                                                                                                 NaN                                                                                                                                                                                                                                          NaN                                                                                                                                                NaN                                                                                       NaN                                                                                                                                        NaN                                             NaN                                                                                                      NaN                                                                                                                 NaN                                                                NaN                   NaN                                                                                                                                                                                                                                   NaN                                                                    NaN                                                                                                          NaN                                 NaN                     NaN                          NaN                                                                            NaN                                                             NaN                                                 NaN             NaN                                                                                                                            0.0\n",
              "\"Petter Mattei's \"\"Love in the Time of Money\"\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money  power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way                                                                                                                                                 or another to the next person                                                                                                                                                                                                                                                but no one seems to know the previous point of contact. Stylishly                                                                                                                                                                                              the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment   as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi                                                                                                  Rosario Dawson                                                                                                                                     Carol Kane                                                                                Michael Imperioli                                                                                                                          Adrian Grenier                                  and the rest of the talented cast                                                                        make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.\"  positive                                                           NaN                   NaN                                                                                                                                                                                                                                   NaN                                                                    NaN                                                                                                          NaN                                 NaN                     NaN                          NaN                                                                            NaN                                                             NaN                                                 NaN             NaN                                                                                                                            0.0\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ... \n",
              "\"I thought this movie did a down right good job. It wasn't as creative or original as the first                                                                                                           but who was expecting it to be. It was a whole lotta fun. the more i think about it the more i like it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   and when it comes out on DVD I'm going to pay the money for it very proudly                                                                                                                                                                                                                            every last cent. Sharon Stone is great                                                                                                                                                                                                                                       she always is                                                                                                                                                                                                                                                  even if her movie is horrible(Catwoman)                                                                                                                                                                                                                                                                                                                             but this movie isn't                                                                                                                                                                                                                         this is one of those movies that will be underrated for its lifetime                                                                               and it will probably become a classic in like 20 yrs. Don't wait for it to be a classic   watch it now and enjoy it. Don't expect a masterpiece                                                                                      or something thats gripping and soul touching   just allow yourself to get out of your life and get yourself involved in theirs.<br /><br />All in all   this movie is entertaining and i recommend people who haven't seen it see it                                        because what the critics and box office say doesn't always count   see it for yourself   you never know                                                                                                                                                                                                                        you might just enjoy it. I tip my hat to this movie<br /><br />8/10\"  positive                                                                                                     NaN                                 NaN                     NaN                          NaN                                                                            NaN                                                             NaN                                                 NaN             NaN                                                                                                                            0.0\n",
              "\"Bad plot                                                                                                                                                                                                 bad dialogue                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             bad acting                                                                                                                                                                                                                                                                                             idiotic directing                                                                                                                                                                                                                                                            the annoying porn groove soundtrack that ran continually over the overacted script                                                                                                                                                                             and a crappy copy of the VHS cannot be redeemed by consuming liquor. Trust me                                                                                                                                                                                                                                                                                       because I stuck this turkey out to the end. It was so pathetically bad all over that I had to figure it was a fourth-rate spoof of Springtime for Hitler.<br /><br />The girl who played Janis Joplin was the only faint spark of interest   and that was only because she could sing better than the original.<br /><br />If you want to watch something similar but a thousand times better   then watch Beyond The Valley of The Dolls.\"                                              negative                                                                                                                                   NaN                                             NaN                                                                                                      NaN                                                                                                                 NaN                                                                NaN                   NaN                                                                                                                                                                                                                                   NaN                                                                    NaN                                                                                                          NaN                                 NaN                     NaN                          NaN                                                                            NaN                                                             NaN                                                 NaN             NaN                                                                                                                            0.0\n",
              "\"I am a Catholic taught in parochial elementary schools by nuns                                                                                                                                           taught by Jesuit priests in high school & college. I am still a practicing Catholic but would not be considered a \"\"good Catholic\"\" in the church's eyes because I don't believe certain things or act certain ways just because the church tells me to.<br /><br />So back to the movie...its bad because two people are killed by this nun who is supposed to be a satire as the embodiment of a female religious figurehead. There is no comedy in that and the satire is not done well by the over acting of Diane Keaton. I never saw the play but if it was very different from this movies then it may be good.<br /><br />At first I thought the gun might be a fake and the first shooting all a plan by the female lead of the four former students as an attempt to demonstrate Sister Mary's emotional and intellectual bigotry of faith. But it turns out the bullets were real and the story has tragedy...the tragedy of loss of life (besides the two former students...the lives of the aborted babies  the life of the student's mom)                                                                                                                                                                                                                                                                         the tragedy of dogmatic authority over love of people                                                                                                                                                                                                                        the tragedy of organized religion replacing true faith in God. This is what is wrong with today's Islam                                                                                                                                                        and yesterday's Judaism and Christianity.\"                                                                                                                                                                                                                                                                                                                         negative                                                                                                                                                                                                                                     NaN                                                                                                                                                NaN                                                                                       NaN                                                                                                                                        NaN                                             NaN                                                                                                      NaN                                                                                                                 NaN                                                                NaN                   NaN                                                                                                                                                                                                                                   NaN                                                                    NaN                                                                                                          NaN                                 NaN                     NaN                          NaN                                                                            NaN                                                             NaN                                                 NaN             NaN                                                                                                                            0.0\n",
              "\"I'm going to have to disagree with the previous comment and side with Maltin on this one. This is a second rate                                                                                          excessively vicious Western that creaks and groans trying to put across its central theme of the Wild West being tamed and kicked aside by the steady march of time. It would like to be in the tradition of \"\"Butch Cassidy and the Sundance Kid\"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      but lacks that film's poignancy and charm. Andrew McLaglen's direction is limp                                                                                                                                                                                                                         and the final 30 minutes or so are a real botch                                                                                                                                                                                                                              with some incomprehensible strategy on the part of heroes Charlton Heston and Chris Mitchum. (Someone give me a holler if you can explain to me why they set that hillside on fire.) There was something callous about the whole treatment of the rape scene   and the woman's reaction afterwards certainly did not ring true. Coburn is plenty nasty as the half breed escaped convict out for revenge                                                                                                                                                                                                                           but all of his fellow escapees are underdeveloped (they're like bowling pins to be knocked down one by one as the story lurches forward). Michael Parks gives one of his typically shifty                                                    lethargic                                                                                                                                          mumbling performances                                                                     but in this case it was appropriate as his modern style sheriff symbolizes the complacency that technological progress can bring about.\"  negative                                        NaN                                                                                                      NaN                                                                                                                 NaN                                                                NaN                   NaN                                                                                                                                                                                                                                   NaN                                                                    NaN                                                                                                          NaN                                 NaN                     NaN                          NaN                                                                            NaN                                                             NaN                                                 NaN             NaN                                                                                                                            0.0\n",
              "\"No one expects the Star Trek movies to be high art                                                                                                                                                       but the fans do expect a movie that is as good as some of the best episodes. Unfortunately                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               this movie had a muddled                                                                                                                                                                                                                                                                               implausible plot that just left me cringing - this is by far the worst of the nine (so far) movies. Even the chance to watch the well known characters interact in another movie can't save this movie - including the goofy scenes with Kirk                                Spock and McCoy at Yosemite.<br /><br />I would say this movie is not worth a rental                                                                                                                                                                           and hardly worth watching                                                                                                                                                                                                                                                                                                                                           however for the True Fan who needs to see all the movies                                                                                                                                                                                     renting this movie is about the only way you'll see it - even the cable channels avoid this movie.\"                                               negative                                                                                  NaN                                                                                                                                        NaN                                             NaN                                                                                                      NaN                                                                                                                 NaN                                                                NaN                   NaN                                                                                                                                                                                                                                   NaN                                                                    NaN                                                                                                          NaN                                 NaN                     NaN                          NaN                                                                            NaN                                                             NaN                                                 NaN             NaN                                                                                                                            0.0\n",
              "Name: similarity, Length: 45999, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right</th>\n",
              "      <th>as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence</th>\n",
              "      <th>which set in right from the word GO. Trust me</th>\n",
              "      <th>this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs</th>\n",
              "      <th>sex or violence. Its is hardcore</th>\n",
              "      <th>in the classic use of the word.&lt;br /&gt;&lt;br /&gt;It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City</th>\n",
              "      <th>an experimental section of the prison where all the cells have glass fronts and face inwards</th>\n",
              "      <th>so privacy is not high on the agenda. Em City is home to many..Aryans</th>\n",
              "      <th>Muslims</th>\n",
              "      <th>gangstas</th>\n",
              "      <th>Latinos</th>\n",
              "      <th>Christians</th>\n",
              "      <th>Italians</th>\n",
              "      <th>Irish and more....so scuffles</th>\n",
              "      <th>death stares</th>\n",
              "      <th>dodgy dealings and shady agreements are never far away.&lt;br /&gt;&lt;br /&gt;I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences</th>\n",
              "      <th>forget charm</th>\n",
              "      <th>forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal</th>\n",
              "      <th>I couldn't say I was ready for it</th>\n",
              "      <th>but as I watched more</th>\n",
              "      <th>I developed a taste for Oz</th>\n",
              "      <th>and got accustomed to the high levels of graphic violence. Not just violence</th>\n",
              "      <th>but injustice (crooked guards who'll be sold out for a nickel</th>\n",
              "      <th>inmates who'll kill on order and get away with it</th>\n",
              "      <th>well mannered</th>\n",
              "      <th>middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting</th>\n",
              "      <th>and sometimes discomforting</th>\n",
              "      <th>sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"\"has got all the polari\"\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries</th>\n",
              "      <th>not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. &lt;br /&gt;&lt;br /&gt;The realism really comes home with the little things: the fantasy of the guard which</th>\n",
              "      <th>rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses</th>\n",
              "      <th>particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\"</th>\n",
              "      <th>positive</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"I thought this was a wonderful way to spend time on a too hot summer weekend</th>\n",
              "      <th>sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic</th>\n",
              "      <th>but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction</th>\n",
              "      <th>I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.&lt;br /&gt;&lt;br /&gt;This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson</th>\n",
              "      <th>in this she managed to tone down her \"\"sexy\"\" image and jumped right into a average</th>\n",
              "      <th>but spirited young woman.&lt;br /&gt;&lt;br /&gt;This may not be the crown jewel of his career</th>\n",
              "      <th>but it was wittier than \"\"Devil Wears Prada\"\" and more interesting than \"\"Superman\"\" a great comedy to go see with friends.\"</th>\n",
              "      <th>positive</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly</th>\n",
              "      <th>Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK</th>\n",
              "      <th>first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing &amp; arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie</th>\n",
              "      <th>and instead i watched a drama with some meaningless thriller spots.&lt;br /&gt;&lt;br /&gt;3 out of 10 just for the well playing parents &amp; descent dialogs. As for the shots with Jake: just ignore them.\"</th>\n",
              "      <th>negative</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"Petter Mattei's \"\"Love in the Time of Money\"\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money</th>\n",
              "      <th>power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a variation on the Arthur Schnitzler's play about the same theme</th>\n",
              "      <th>the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way</th>\n",
              "      <th>or another to the next person</th>\n",
              "      <th>but no one seems to know the previous point of contact. Stylishly</th>\n",
              "      <th>the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.&lt;br /&gt;&lt;br /&gt;The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment</th>\n",
              "      <th>as one discerns is the case with most of the people we encounter.&lt;br /&gt;&lt;br /&gt;The acting is good under Mr. Mattei's direction. Steve Buscemi</th>\n",
              "      <th>Rosario Dawson</th>\n",
              "      <th>Carol Kane</th>\n",
              "      <th>Michael Imperioli</th>\n",
              "      <th>Adrian Grenier</th>\n",
              "      <th>and the rest of the talented cast</th>\n",
              "      <th>make these characters come alive.&lt;br /&gt;&lt;br /&gt;We wish Mr. Mattei good luck and await anxiously for his next work.\"</th>\n",
              "      <th>positive</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"I thought this movie did a down right good job. It wasn't as creative or original as the first</th>\n",
              "      <th>but who was expecting it to be. It was a whole lotta fun. the more i think about it the more i like it</th>\n",
              "      <th>and when it comes out on DVD I'm going to pay the money for it very proudly</th>\n",
              "      <th>every last cent. Sharon Stone is great</th>\n",
              "      <th>she always is</th>\n",
              "      <th>even if her movie is horrible(Catwoman)</th>\n",
              "      <th>but this movie isn't</th>\n",
              "      <th>this is one of those movies that will be underrated for its lifetime</th>\n",
              "      <th>and it will probably become a classic in like 20 yrs. Don't wait for it to be a classic</th>\n",
              "      <th>watch it now and enjoy it. Don't expect a masterpiece</th>\n",
              "      <th>or something thats gripping and soul touching</th>\n",
              "      <th>just allow yourself to get out of your life and get yourself involved in theirs.&lt;br /&gt;&lt;br /&gt;All in all</th>\n",
              "      <th>this movie is entertaining and i recommend people who haven't seen it see it</th>\n",
              "      <th>because what the critics and box office say doesn't always count</th>\n",
              "      <th>see it for yourself</th>\n",
              "      <th>you never know</th>\n",
              "      <th>you might just enjoy it. I tip my hat to this movie&lt;br /&gt;&lt;br /&gt;8/10\"</th>\n",
              "      <th>positive</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"Bad plot</th>\n",
              "      <th>bad dialogue</th>\n",
              "      <th>bad acting</th>\n",
              "      <th>idiotic directing</th>\n",
              "      <th>the annoying porn groove soundtrack that ran continually over the overacted script</th>\n",
              "      <th>and a crappy copy of the VHS cannot be redeemed by consuming liquor. Trust me</th>\n",
              "      <th>because I stuck this turkey out to the end. It was so pathetically bad all over that I had to figure it was a fourth-rate spoof of Springtime for Hitler.&lt;br /&gt;&lt;br /&gt;The girl who played Janis Joplin was the only faint spark of interest</th>\n",
              "      <th>and that was only because she could sing better than the original.&lt;br /&gt;&lt;br /&gt;If you want to watch something similar but a thousand times better</th>\n",
              "      <th>then watch Beyond The Valley of The Dolls.\"</th>\n",
              "      <th>negative</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"I am a Catholic taught in parochial elementary schools by nuns</th>\n",
              "      <th>taught by Jesuit priests in high school &amp; college. I am still a practicing Catholic but would not be considered a \"\"good Catholic\"\" in the church's eyes because I don't believe certain things or act certain ways just because the church tells me to.&lt;br /&gt;&lt;br /&gt;So back to the movie...its bad because two people are killed by this nun who is supposed to be a satire as the embodiment of a female religious figurehead. There is no comedy in that and the satire is not done well by the over acting of Diane Keaton. I never saw the play but if it was very different from this movies then it may be good.&lt;br /&gt;&lt;br /&gt;At first I thought the gun might be a fake and the first shooting all a plan by the female lead of the four former students as an attempt to demonstrate Sister Mary's emotional and intellectual bigotry of faith. But it turns out the bullets were real and the story has tragedy...the tragedy of loss of life (besides the two former students...the lives of the aborted babies</th>\n",
              "      <th>the life of the student's mom)</th>\n",
              "      <th>the tragedy of dogmatic authority over love of people</th>\n",
              "      <th>the tragedy of organized religion replacing true faith in God. This is what is wrong with today's Islam</th>\n",
              "      <th>and yesterday's Judaism and Christianity.\"</th>\n",
              "      <th>negative</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"I'm going to have to disagree with the previous comment and side with Maltin on this one. This is a second rate</th>\n",
              "      <th>excessively vicious Western that creaks and groans trying to put across its central theme of the Wild West being tamed and kicked aside by the steady march of time. It would like to be in the tradition of \"\"Butch Cassidy and the Sundance Kid\"\"</th>\n",
              "      <th>but lacks that film's poignancy and charm. Andrew McLaglen's direction is limp</th>\n",
              "      <th>and the final 30 minutes or so are a real botch</th>\n",
              "      <th>with some incomprehensible strategy on the part of heroes Charlton Heston and Chris Mitchum. (Someone give me a holler if you can explain to me why they set that hillside on fire.) There was something callous about the whole treatment of the rape scene</th>\n",
              "      <th>and the woman's reaction afterwards certainly did not ring true. Coburn is plenty nasty as the half breed escaped convict out for revenge</th>\n",
              "      <th>but all of his fellow escapees are underdeveloped (they're like bowling pins to be knocked down one by one as the story lurches forward). Michael Parks gives one of his typically shifty</th>\n",
              "      <th>lethargic</th>\n",
              "      <th>mumbling performances</th>\n",
              "      <th>but in this case it was appropriate as his modern style sheriff symbolizes the complacency that technological progress can bring about.\"</th>\n",
              "      <th>negative</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"No one expects the Star Trek movies to be high art</th>\n",
              "      <th>but the fans do expect a movie that is as good as some of the best episodes. Unfortunately</th>\n",
              "      <th>this movie had a muddled</th>\n",
              "      <th>implausible plot that just left me cringing - this is by far the worst of the nine (so far) movies. Even the chance to watch the well known characters interact in another movie can't save this movie - including the goofy scenes with Kirk</th>\n",
              "      <th>Spock and McCoy at Yosemite.&lt;br /&gt;&lt;br /&gt;I would say this movie is not worth a rental</th>\n",
              "      <th>and hardly worth watching</th>\n",
              "      <th>however for the True Fan who needs to see all the movies</th>\n",
              "      <th>renting this movie is about the only way you'll see it - even the cable channels avoid this movie.\"</th>\n",
              "      <th>negative</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45999 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(df_imdb['similarity'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XHuLz7HmcVm",
        "outputId": "4056e1eb-4994-414b-e7a4-46b61db5ddb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9765613282392966"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}